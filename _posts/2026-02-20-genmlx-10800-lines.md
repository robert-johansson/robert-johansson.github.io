---
layout: post
title: "GenMLX Reaches 10,800 Lines"
date: 2026-02-20 12:15:00 +0100
categories: [probabilistic-programming]
tags: [clojurescript, mlx, gen, apple-silicon, bayesian]
share: false
---

[Yesterday]({% post_url 2026-02-19-genmlx-progress %}) GenMLX was ~5,700 lines. Today it's ~10,800 --- 23 commits, 31 files changed, +3,933 / -254 lines. Here's what happened.

## Vectorized everything

The biggest theme: taking the [broadcasting-based vectorization]({% post_url 2026-02-19-genmlx-progress %}) and pushing it through the entire inference stack.

**Batch sampling coverage.** Native `dist-sample-n` now covers nearly every distribution. Two rounds of work added discrete-uniform, geometric, categorical, multivariate-normal, binomial, and student-t, then a vectorized Marsaglia-Tsang implementation for gamma (with Ahrens-Dieter for alpha < 1) that unlocks beta, inverse-gamma, and dirichlet batch sampling. Only poisson remains sequential --- it's inherently so.

**Vectorized splice.** Previously unsupported. Now DynamicGF sub-generative-functions run under batched handlers, with nested splice (3+ levels) working correctly.

**Vectorized MCMC.** N independent MH chains run in parallel via MLX broadcasting. This enables parallel tempering and computing R-hat convergence diagnostics from a single call.

**Vectorized SMC sweep.** Multi-step batched particle filtering --- the model body runs *once* per timestep for all N particles. 24.5x speedup over sequential SMC (50 particles, 5 steps).

**Compiled MH chain.** The entire MH accept/reject loop compiled into a single Metal program via `mx/compile-fn`. Similarly, the VI optimization loop compiles into a single Metal dispatch.

## VIMCO and ADEV gradient estimators

Two new gradient estimation methods for variational inference:

**VIMCO** --- multi-sample variational objective with leave-one-out baseline computation and per-sample gradient weighting, giving tighter gradient estimates than single-sample ELBO.

**ADEV** --- sound automatic differentiation of expected values (from the POPL 2023 paper). Integrates reparameterization and score-function estimators with `mx/grad`, with an ADEV handler for tracing through stochastic computation graphs.

## Recurse combinator

A fixed-point combinator for recursive generative functions --- random trees, linked lists, stochastic grammars. Full GFI support: simulate, generate, update, regenerate, project, edit, update-with-diffs. This brings the combinator count to 9.

## Complete GFI coverage on all combinators

Mask and Contramap/Dimap previously only supported simulate and generate. Now all combinators have complete GFI coverage --- update, regenerate, and update-with-diffs across the board.

## Incremental computation for sequential models

Unfold and Scan now store per-step scores (and carries for Scan) as trace metadata. During `update`, the system detects the earliest constrained step and skips the unchanged prefix, replaying old choices and scores from metadata. This turns O(T^2) total work for SMC on time series into O(T).

## Custom gradient generative functions

`CustomGradientGF` allows user-supplied forward/backward passes. The `IHasArgumentGrads` protocol on generative functions indicates which arguments are differentiable, used by gradient-based inference to avoid unnecessary gradient computation.

## Neural network integration

`NeuralNetGF` wraps MLX `nn.Module` as a deterministic generative function with simulate, generate, assess, and propose. Layer constructors (linear, sequential, relu, gelu, etc.) and training utilities using MLX's native `nn.valueAndGrad`.

Building on this, an amortized inference module provides reparameterized ELBO training (VAE-style) and neural importance sampling using a trained guide as proposal.

## The numbers

| Metric | Value |
|---|---|
| Commits today | 23 |
| Lines added | 3,933 |
| Lines removed | 254 |
| New source files | 5 |
| New test files | 9 |
| TODO items completed | 17 (24 to 41 of 66) |

The codebase has roughly doubled in a day. The remaining 25 TODO items are mostly in phases I haven't touched yet --- serialization, parallel tempering as a first-class algorithm, and the static DSL.

The code is at [github.com/robert-johansson/genmlx](https://github.com/robert-johansson/genmlx).
